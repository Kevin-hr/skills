#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
=====================================================================
ç”µå•†è€æ¿çƒ­ç‚¹æŒ–æ˜æœº - çœŸå®æ•°æ®æºç‰ˆ V2.0
=====================================================================

ã€ç»éªŒæ€»ç»“ - 2026-02-03ã€‘

## æ•°æ®æºè°ƒç ”è¿‡ç¨‹

### å¯ç”¨çš„å…è´¹æ•°æ®æºï¼ˆæŒ‰å¯ç”¨æ€§æ’åºï¼‰

| æ•°æ®æº | ç±»å‹ | å¯ç”¨æ€§ | å¤‡æ³¨ |
|:---|:---|:---:|:---|
| è™å—…ç½‘ | ç½‘é¡µæŠ“å– | âœ… å¯ç”¨ | è´¢ç»/ç§‘æŠ€/å•†ä¸šçƒ­ç‚¹ |
| 36æ°ª | ç½‘é¡µæŠ“å– | âœ… å¯ç”¨ | åˆ›ä¸š/ç§‘æŠ€/å•†ä¸šçƒ­ç‚¹ |
| çŸ¥ä¹çƒ­æ¦œ | ç½‘é¡µæŠ“å– | âš ï¸ éƒ¨åˆ†å¯ç”¨ | éœ€è¦ç™»å½• |
| ç™¾åº¦çƒ­æœ | ç½‘é¡µæŠ“å– | âŒ åŠ¨æ€é¡µé¢ | JSæ¸²æŸ“ï¼Œæ— æ³•ç›´æ¥æŠ“å– |
| å¾®åšçƒ­æœ | API | âŒ 403 | éœ€è¦ç™»å½•/é¢‘ç‡é™åˆ¶ |
| ä»Šæ—¥å¤´æ¡ | ç½‘é¡µæŠ“å– | âŒ åŠ¨æ€é¡µé¢ | JSæ¸²æŸ“ |

### ç»“è®º
âœ… **å¯ç”¨æ–¹æ¡ˆ**: è™å—…ç½‘ + 36æ°ª ç½‘é¡µæŠ“å–
âœ… **å¤‡ç”¨æ–¹æ¡ˆ**: å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ ‡é¢˜

### æŠ“å–éš¾ç‚¹
1. åŠ¨æ€é¡µé¢ï¼ˆç™¾åº¦/å¤´æ¡/å¾®åšï¼‰ï¼šJSæ¸²æŸ“ï¼Œrequestsæ— æ³•è·å–
2. APIé™åˆ¶ï¼šå¾®åš/çŸ¥ä¹éœ€è¦ç™»å½•/é¢‘ç‡é™åˆ¶
3. åçˆ¬è™«ï¼šéƒ¨åˆ†ç½‘ç«™æœ‰IP/UAæ£€æµ‹

### è§£å†³æ–¹æ¡ˆ
1. é™æ€é¡µé¢ä¼˜å…ˆï¼šè™å—…ã€36æ°ªé¦–é¡µå¯æŠ“
2. è§£ææŠ€å·§ï¼šBeautifulSoup + æ­£åˆ™
3. é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨é¢„è®¾çƒ­ç‚¹è¯åº“ï¼ˆå½“åœ¨çº¿ä¸å¯ç”¨æ—¶ï¼‰

=====================================================================
"""

import argparse
import json
import re
import sys
from datetime import datetime
from typing import List, Dict, Optional

try:
    import requests
    from bs4 import BeautifulSoup
    HAS_DEPS = True
except ImportError:
    HAS_DEPS = False


# ==================== ç”µå•†è€æ¿ç—›ç‚¹æ˜ å°„è¡¨ ====================
"""
ã€æ ¸å¿ƒç»éªŒã€‘å°†é€šç”¨çƒ­ç‚¹æ˜ å°„åˆ°ç”µå•†è€æ¿ç—›ç‚¹çš„é€»è¾‘

ä¸ºä»€ä¹ˆéœ€è¦æ˜ å°„ï¼Ÿ
- é€šç”¨çƒ­ç‚¹ï¼ˆé»„é‡‘/æˆ¿ä»·/AIï¼‰å’Œè€æ¿ç—›ç‚¹ï¼ˆåˆ©æ¶¦/æµé‡/æˆæœ¬ï¼‰ä¹‹é—´éœ€è¦å»ºç«‹å…³è”
- åŒä¸€ä¸ªçƒ­ç‚¹å¯ä»¥ä»ä¸åŒè§’åº¦è§£è¯»

æ˜ å°„è§„åˆ™ï¼š
1. ç›´æ¥åŒ¹é…ï¼šçƒ­ç‚¹å«æœ‰å…³é”®è¯ â†’ ç›´æ¥æ˜ å°„
2. é—´æ¥å…³è”ï¼šçƒ­ç‚¹ä¸»é¢˜ â†’ æ¨å¯¼ç—›ç‚¹
"""

PAIN_POINT_MAPPING = {
    "åˆ©æ¶¦è–„": [
        "èµšé’±", "ç›ˆåˆ©", "åˆ©æ¶¦", "äºæŸ", "äºæœ¬", "æ”¶å…¥", "è¥æ”¶",
        "ç”Ÿæ„", "è®¢å•", "é”€å”®é¢", "GMV", "å®¢å•ä»·"
    ],
    "æµé‡è´µ": [
        "æµé‡", "è·å®¢", "æ¨å¹¿", "å¹¿å‘Š", "è¥é”€", "æ›å…‰",
        "è½¬åŒ–", "ç‚¹å‡»", "è¯¢ç›˜", "å¼•æµ"
    ],
    "æˆæœ¬é«˜": [
        "æˆæœ¬", "è´¹ç”¨", "æ¶¨ä»·", "ä»·æ ¼", "æˆ¿ç§Ÿ", "äººå·¥", "å·¥èµ„",
        "ç§Ÿé‡‘", "ç‰©æµ", "è¿è´¹", "åŸææ–™", "å…³ç¨"
    ],
    "è½¬åŒ–éš¾": [
        "è½¬åŒ–", "é”€å”®", "è´­ä¹°", "ä¸‹å•", "æˆäº¤", "æˆäº¤ç‡",
        "å¤è´­", "ç•™å­˜", "æ´»è·ƒ"
    ],
    "å¹³å°å‹æ¦¨": [
        "å¹³å°", "è§„åˆ™", "æŠ½æˆ", "ä½£é‡‘", "å°å·", "ç›‘ç®¡",
        "æ”¿ç­–", "åˆè§„", "å¤„ç½š", "é™æµ"
    ],
    "AIç„¦è™‘": [
        "AI", "äººå·¥æ™ºèƒ½", "è‡ªåŠ¨åŒ–", "æ™ºèƒ½", "æ›¿ä»£", "è£å‘˜",
        "æ™ºèƒ½ä½“", "å¤§æ¨¡å‹", "æœºå™¨äºº", "æ— äºº"
    ],
    "èµ„é‡‘å‹åŠ›": [
        "èµ„é‡‘", "èèµ„", "è´·æ¬¾", "è´¦æœŸ", "å›æ¬¾", "ç°é‡‘æµ",
        "å€ºåŠ¡", "è¿çº¦", "ç ´äº§", "æŠ•èµ„", "å‹Ÿèµ„"
    ],
    "æ¶ˆè´¹é™çº§": [
        "æ¶ˆè´¹", "ç»æµ", "é™çº§", "é€šç¼©", "çœé’±", "ä½ä»·",
        "æ€§ä»·æ¯”", "æŠ˜æ‰£", "ä¾¿å®œ"
    ],
}

# ç”µå•†ç›¸å…³åŠ æƒå…³é”®è¯ï¼ˆå‡ºç°åˆ™åŠ æƒï¼‰
ECOMMERCE_KEYWORDS = [
    "ç”µå•†", "è·¨å¢ƒ", "é›¶å”®", "å•†ä¸š", "ä¼ä¸š", "è€æ¿",
    "å•†å®¶", "å•†æˆ·", "å–å®¶", "å¤©çŒ«", "æ·˜å®", "äº¬ä¸œ",
    "æ‹¼å¤šå¤š", "äºšé©¬é€Š", "TikTok", "Shopee"
]


class RealHotspotMiner:
    """
    ç”µå•†è€æ¿çƒ­ç‚¹æŒ–æ˜æœº

    ã€ä½¿ç”¨ç»éªŒã€‘
    1. ä¼˜å…ˆæŠ“å–è™å—…ç½‘ï¼ˆè´¢ç»äº§ä¸šä¸ºä¸»ï¼‰
    2. å…¶æ¬¡36æ°ªï¼ˆç§‘æŠ€åˆ›ä¸šä¸ºä¸»ï¼‰
    3. ä¸¤è€…çš„å¹¶é›†è¦†ç›–å¤§éƒ¨åˆ†å•†ä¸šçƒ­ç‚¹
    """

    def __init__(self, timeout: int = 30):
        self.timeout = timeout
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        })

    def _fetch_page(self, url: str) -> Optional[str]:
        """é€šç”¨é¡µé¢è·å–"""
        try:
            resp = self.session.get(url, timeout=self.timeout)
            resp.encoding = "utf-8"
            return resp.text
        except Exception as e:
            print(f"è¯·æ±‚å¤±è´¥ [{url}]: {e}", file=sys.stderr)
            return None

    def _extract_titles_from_soup(self, html: str, selector: str = "a") -> List[str]:
        """ä»HTMLä¸­æå–æ ‡é¢˜"""
        if not html:
            return []
        soup = BeautifulSoup(html, "html.parser")
        titles = []
        for elem in soup.find_all(selector):
            text = elem.get_text(strip=True)
            if 5 <= len(text) <= 60:
                titles.append(text)
        return titles

    def _calculate_pain_score(self, title: str) -> Dict:
        """
        ã€æ ¸å¿ƒç®—æ³•ã€‘è®¡ç®—çƒ­ç‚¹ä¸è€æ¿ç—›ç‚¹çš„å…³è”åº¦

        è¯„åˆ†é€»è¾‘ï¼š
        - ç›´æ¥å‘½ä¸­ç—›ç‚¹å…³é”®è¯ï¼š+20-25åˆ†
        - å‘½ä¸­ç”µå•†ç›¸å…³è¯ï¼š+15åˆ†
        - æœ€é«˜100åˆ†å°é¡¶
        """
        score = 0
        matched_tags = []

        # ç—›ç‚¹åŒ¹é…
        for pain_point, keywords in PAIN_POINT_MAPPING.items():
            for kw in keywords:
                if kw in title:
                    if pain_point in ["åˆ©æ¶¦è–„", "æµé‡è´µ", "æˆæœ¬é«˜"]:
                        score += 25
                    elif pain_point in ["å¹³å°å‹æ¦¨", "èµ„é‡‘å‹åŠ›", "AIç„¦è™‘"]:
                        score += 22
                    else:
                        score += 15
                    matched_tags.append(pain_point)
                    break

        # ç”µå•†åŠ æƒ
        for kw in ECOMMERCE_KEYWORDS:
            if kw in title:
                score += 15
                break

        # æƒ©ç½šï¼šéç›¸å…³å†…å®¹
        exclude_patterns = ["å¨±ä¹", "æ˜æ˜Ÿ", "å…«å¦", "ç»¯é—»", "æ‹æƒ…", "ç¦»å©š"]
        for p in exclude_patterns:
            if p in title:
                score = max(score - 30, 0)
                break

        return {
            "score": min(score, 100),
            "tags": matched_tags[:3] if matched_tags else ["ä¸€èˆ¬"]
        }

    def mine_huxiu(self, limit: int = 20) -> List[Dict]:
        """
        ã€æ•°æ®æº1ã€‘è™å—…ç½‘çƒ­ç‚¹æŒ–æ˜

        URL: https://www.huxiu.com
        ç‰¹ç‚¹ï¼šè´¢ç»ã€äº§ä¸šã€å•†ä¸šåˆ†æä¸ºä¸»ï¼Œé€‚åˆæ‰¾ç»æµè¶‹åŠ¿ç±»çƒ­ç‚¹
        """
        url = "https://www.huxiu.com"
        html = self._fetch_page(url)
        if not html:
            return []

        soup = BeautifulSoup(html, "html.parser")
        results = []

        # è™å—…æ–‡ç« é“¾æ¥é€šå¸¸åŒ…å« /article/
        for a in soup.find_all("a", href=re.compile(r'/article/\d+')):
            title = a.get_text(strip=True)
            if title and 5 <= len(title) <= 50:
                # è¿‡æ»¤æ— æ•ˆæ ‡é¢˜
                if not any(e in title for e in ["è§†é¢‘", "ç›´æ’­", "ä¸“é¢˜", "æ´»åŠ¨", "ä¸“æ "]):
                    pain_info = self._calculate_pain_score(title)
                    results.append({
                        "keyword": title,
                        "source": "è™å—…",
                        "url": "https://www.huxiu.com" + a.get("href", ""),
                        **pain_info
                    })

        # å»é‡
        seen = set()
        unique = []
        for item in results:
            if item["keyword"] not in seen:
                seen.add(item["keyword"])
                unique.append(item)

        return unique[:limit]

    def mine_36kr(self, limit: int = 15) -> List[Dict]:
        """
        ã€æ•°æ®æº2ã€‘36æ°ªçƒ­ç‚¹æŒ–æ˜

        URL: https://36kr.com/hot-news
        ç‰¹ç‚¹ï¼šç§‘æŠ€ã€åˆ›ä¸šã€å•†ä¸šä¸ºä¸»ï¼Œé€‚åˆæ‰¾AI/ç§‘æŠ€ç±»çƒ­ç‚¹
        """
        url = "https://36kr.com/hot-news"
        html = self._fetch_page(url)
        if not html:
            return []

        soup = BeautifulSoup(html, "html.parser")
        results = []

        # 36æ°ªæ–‡ç« é“¾æ¥é€šå¸¸åŒ…å« /p/
        for a in soup.find_all("a", href=re.compile(r'/p/\d+')):
            title = a.get_text(strip=True)
            if title and 5 <= len(title) <= 50:
                pain_info = self._calculate_pain_score(title)
                results.append({
                    "keyword": title,
                    "source": "36æ°ª",
                    "url": "https://36kr.com" + a.get("href", ""),
                    **pain_info
                })

        # å»é‡
        seen = set()
        unique = []
        for item in results:
            if item["keyword"] not in seen:
                seen.add(item["keyword"])
                unique.append(item)

        return unique[:limit]

    def mine_all(self, limit: int = 20) -> Dict:
        """
        ã€ä¸»å…¥å£ã€‘èšåˆæ‰€æœ‰æ•°æ®æº

        è¿”å›æ ¼å¼ï¼š
        {
            "success": True,
            "fetch_time": "2026-02-03 20:30:00",
            "sources": ["è™å—…", "36æ°ª"],
            "data": [...]
        }
        """
        huxiu_data = self.mine_huxiu(limit=limit)
        kr36_data = self.mine_36kr(limit=limit)

        all_data = huxiu_data + kr36_data

        if not all_data:
            return {
                "success": False,
                "error": "æœªèƒ½è·å–åˆ°ä»»ä½•çƒ­ç‚¹æ•°æ®",
                "suggestion": "è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•"
            }

        # æŒ‰åˆ†æ•°æ’åº
        all_data.sort(key=lambda x: x["score"], reverse=True)

        return {
            "success": True,
            "fetch_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "sources": ["è™å—…", "36æ°ª"],
            "total": len(all_data),
            "data": all_data[:limit]
        }

    def analyze_keyword(self, keyword: str) -> Dict:
        """åˆ†æä»»æ„å…³é”®è¯çš„ç—›ç‚¹å…³è”"""
        pain_info = self._calculate_pain_score(keyword)
        return {
            "keyword": keyword,
            "pain_score": pain_info["score"],
            "tags": pain_info["tags"]
        }


def format_output(data: Dict, user_persona: bool = True) -> str:
    """æ ¼å¼åŒ–è¾“å‡º"""
    if not data.get("success"):
        return f"é”™è¯¯: {data.get('error', 'æœªçŸ¥é”™è¯¯')}\nå»ºè®®: {data.get('suggestion', '')}"

    lines = []
    lines.append("=" * 70)
    if user_persona:
        lines.append("  ç”µå•†è€æ¿å…³è”çƒ­ç‚¹é€Ÿé€’")
        lines.append("  ç”¨æˆ·ç”»åƒ: [è¢«åˆ©æ¶¦æä½å–‰å’™çš„è€æ¿]")
    else:
        lines.append("  ç”µå•†çƒ­ç‚¹æŒ–æ˜ç»“æœ")
    lines.append("=" * 70)
    lines.append(f"\nâ° {data['fetch_time']}")
    lines.append(f"ğŸ“¡ æ•°æ®æ¥æº: {', '.join(data['sources'])}")
    lines.append("-" * 70)

    for i, item in enumerate(data.get("data", []), 1):
        tags = item.get("tags", [])[:2]
        score = item.get("score", 0)
        bars = "â–ˆ" * (score // 10) + "â–‘" * (10 - score // 10)
        lines.append(f"\n{i:2d}. {item['keyword']}")
        lines.append(f"    æ¥æº: {item['source']}")
        lines.append(f"    å…³è”ç—›ç‚¹: {', '.join(tags) if tags else 'ä¸€èˆ¬'}")
        lines.append(f"    ç›¸å…³åº¦: {bars} {score}åˆ†")

    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(
        description="ç”µå•†è€æ¿çƒ­ç‚¹æŒ–æ˜æœº - çœŸå®æ•°æ®æº",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ã€ä½¿ç”¨ç»éªŒã€‘
æ•°æ®æºï¼š
  - è™å—…ç½‘ (https://www.huxiu.com) â†’ è´¢ç»/äº§ä¸š/å•†ä¸š
  - 36æ°ª (https://36kr.com/hot-news) â†’ ç§‘æŠ€/åˆ›ä¸š/å•†ä¸š

ç¤ºä¾‹ï¼š
  python ecommerce_hotspot_miner.py              # è·å–å…¨éƒ¨çƒ­ç‚¹
  python ecommerce_hotspot_miner.py --limit 10   # TOP10
  python ecommerce_hotspot_miner.py --json        # JSONæ ¼å¼
  python ecommerce_hotspot_miner.py -a "å…‰ä¼"     # åˆ†æå…³é”®è¯

æ³¨æ„äº‹é¡¹ï¼š
  - ç™¾åº¦çƒ­æœ/å¾®åšçƒ­æœéœ€è¦ç™»å½•ï¼Œæ— æ³•ç›´æ¥æŠ“å–
  - å¦‚é‡ç½‘ç»œé—®é¢˜ï¼Œä¼šè¿”å›ç©ºæ•°æ®
        """
    )

    parser.add_argument("-l", "--limit", type=int, default=20,
                        help="è¿”å›æ•°é‡")
    parser.add_argument("-f", "--format", default="text",
                        choices=["text", "json"],
                        help="è¾“å‡ºæ ¼å¼")
    parser.add_argument("-o", "--output",
                        help="è¾“å‡ºåˆ°æ–‡ä»¶")
    parser.add_argument("-a", "--analyze",
                        help="åˆ†æå•ä¸ªå…³é”®è¯")
    parser.add_argument("-q", "--quiet", action="store_true",
                        help="ç®€æ´æ¨¡å¼ï¼ˆä¸æ˜¾ç¤ºç”¨æˆ·ç”»åƒï¼‰")

    args = parser.parse_args()

    if not HAS_DEPS:
        print("è¯·å®‰è£…ä¾èµ–: pip install requests beautifulsoup4")
        sys.exit(1)

    miner = RealHotspotMiner()

    # åˆ†æå…³é”®è¯æ¨¡å¼
    if args.analyze:
        result = miner.analyze_keyword(args.analyze)
        output = json.dumps(result, ensure_ascii=False, indent=2)
        print(output)
        if args.output:
            with open(args.output, "w", encoding="utf-8") as f:
                f.write(output)
        return

    # æ­£å¸¸è·å–
    result = miner.mine_all(limit=args.limit)

    if args.format == "json":
        output = json.dumps(result, ensure_ascii=False, indent=2)
    else:
        output = format_output(result, not args.quiet)

    print(output)

    if args.output:
        with open(args.output, "w", encoding="utf-8") as f:
            f.write(output)
        print(f"\nå·²ä¿å­˜åˆ°: {args.output}")


if __name__ == "__main__":
    main()
