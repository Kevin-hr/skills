#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç™¾åº¦çƒ­æœè·å–å·¥å…·
æ”¯æŒï¼šç½‘é¡µè§£æ + APIåŒé€šé“

æ³¨æ„ï¼šç™¾åº¦NLPçƒ­ç‚¹APIå¤„äºé‚€æµ‹çŠ¶æ€ï¼Œéœ€å®˜æ–¹æˆæƒ
      æœ¬å·¥å…·é»˜è®¤ä½¿ç”¨ç½‘é¡µè§£ææ–¹æ¡ˆ
"""

import argparse
import json
import re
import time
import sys
from datetime import datetime
from typing import List, Dict, Optional

try:
    import requests
    from bs4 import BeautifulSoup
except ImportError:
    print("è¯·å®‰è£…ä¾èµ–: pip install requests beautifulsoup4 lxml")
    sys.exit(1)


# ç™¾åº¦çƒ­æœæ¦œå•åˆ†ç±»æ˜ å°„
CATEGORY_MAP = {
    "hot": {"name": "çƒ­æœæ¦œ", "url": "https://top.baidu.com/board"},
    "domestic": {"name": "å›½å†…æ¦œ", "url": "https://top.baidu.com/board?category=domestic"},
    "abroad": {"name": "å›½é™…æ¦œ", "url": "https://top.baidu.com/board?category=abroad"},
    "finance": {"name": "è´¢ç»æ¦œ", "url": "https://top.baidu.com/board?category=finance"},
    "sports": {"name": "ä½“è‚²æ¦œ", "url": "https://top.baidu.com/board?category=sports"},
    "entertainment": {"name": "å¨±ä¹æ¦œ", "url": "https://top.baidu.com/board?category=entertainment"},
    "education": {"name": "æ•™è‚²æ¦œ", "url": "https://top.baidu.com/board?category=education"},
    "tech": {"name": "ç§‘æŠ€æ¦œ", "url": "https://top.baidu.com/board?category=tech"},
    "game": {"name": "æ¸¸æˆæ¦œ", "url": "https://top.baidu.com/board?category=game"},
    "car": {"name": "æ±½è½¦æ¦œ", "url": "https://top.baidu.com/board?category=car"},
    "estate": {"name": "æˆ¿äº§æ¦œ", "url": "https://top.baidu.com/board?category=estate"},
    "travel": {"name": "æ—…æ¸¸æ¦œ", "url": "https://top.baidu.com/board?category=travel"},
}

# è¯·æ±‚å¤´
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
}


class BaiduHotSearch:
    """ç™¾åº¦çƒ­æœè·å–å™¨"""

    def __init__(self, timeout: int = 30):
        self.timeout = timeout
        self.session = requests.Session()
        self.session.headers.update(HEADERS)

    def _fetch_page(self, url: str) -> Optional[str]:
        """è·å–é¡µé¢HTML"""
        try:
            response = self.session.get(url, timeout=self.timeout)
            response.encoding = "utf-8"
            return response.text
        except requests.RequestException as e:
            print(f"è¯·æ±‚å¤±è´¥: {e}")
            return None

    def _parse_hot_list(self, html: str, limit: int = 20) -> List[Dict]:
        """è§£æçƒ­æœåˆ—è¡¨"""
        soup = BeautifulSoup(html, "lxml")
        results = []

        # æ–¹æ³•1: æŸ¥æ‰¾ script ä¸­çš„ JSON æ•°æ®
        scripts = soup.find_all("script")
        for script in scripts:
            text = script.string or ""
            if "feeds" in text or "hotList" in text or "board-data" in text:
                # å°è¯•æå–JSONæ•°æ®
                json_match = re.search(r'\[.*\]', text, re.DOTALL)
                if json_match:
                    try:
                        data = json.loads(json_match.group())
                        for i, item in enumerate(data[:limit]):
                            results.append({
                                "rank": i + 1,
                                "keyword": item.get("keyword", ""),
                                "heat": item.get("heat", 0),
                                "url": f"https://baidu.com/s?word={item.get('keyword', '')}",
                                "source": "baidu-api"
                            })
                        return results
                    except json.JSONDecodeError:
                        pass

        # æ–¹æ³•2: è§£æDOMç»“æ„
        items = soup.select(".theme-hot, .list-item, .hot-item, .keyword-item")
        for i, item in enumerate(items[:limit]):
            keyword_elem = item.select_one(".keyword, .title, a")
            heat_elem = item.select_one(".heat, .num, .index")

            keyword = keyword_elem.get_text(strip=True) if keyword_elem else ""
            heat_text = heat_elem.get_text(strip=True) if heat_elem else "0"

            # æå–æ•°å­—çƒ­åº¦
            heat_match = re.search(r'(\d+\.?\d*)', heat_text)
            heat = float(heat_match.group(1)) if heat_match else 0

            if keyword:
                results.append({
                    "rank": i + 1,
                    "keyword": keyword,
                    "heat": heat,
                    "url": f"https://baidu.com/s?word={keyword}",
                    "source": "baidu-html"
                })

        # æ–¹æ³•3: é€šç”¨è§£æï¼ˆå…œåº•æ–¹æ¡ˆï¼‰
        if not results:
            all_links = soup.find_all("a", href=re.compile(r'/s\?word='))
            seen = set()
            for i, link in enumerate(all_links[:limit]):
                keyword = link.get_text(strip=True)
                if keyword and keyword not in seen and len(keyword) > 1:
                    seen.add(keyword)
                    results.append({
                        "rank": i + 1,
                        "keyword": keyword,
                        "heat": 0,
                        "url": link.get("href", ""),
                        "source": "baidu-link"
                    })

        return results

    def get_hot_list(self, category: str = "hot", limit: int = 20) -> Dict:
        """è·å–çƒ­æœæ¦œå•"""
        if category not in CATEGORY_MAP:
            return {
                "success": False,
                "error": f"ä¸æ”¯æŒçš„åˆ†ç±»: {category}",
                "available_categories": list(CATEGORY_MAP.keys())
            }

        cat_info = CATEGORY_MAP[category]
        url = cat_info["url"]

        print(f"æ­£åœ¨è·å– {cat_info['name']} ...")

        html = self._fetch_page(url)
        if not html:
            return {
                "success": False,
                "error": "é¡µé¢è·å–å¤±è´¥",
                "category": cat_info["name"]
            }

        data = self._parse_hot_list(html, limit)

        if not data:
            return {
                "success": False,
                "error": "æ•°æ®è§£æå¤±è´¥ï¼Œå¯èƒ½é¡µé¢ç»“æ„å·²å˜åŒ–",
                "category": cat_info["name"]
            }

        return {
            "success": True,
            "category": cat_info["name"],
            "fetch_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "total": len(data),
            "data": data
        }

    def search_keyword_trend(self, keyword: str) -> Dict:
        """æœç´¢å…³é”®è¯çƒ­åº¦è¶‹åŠ¿"""
        # ç™¾åº¦æŒ‡æ•°éœ€è¦ç™»å½•ï¼Œè¿™é‡Œæä¾›æœç´¢ç»“æœé¡µä½œä¸ºæ›¿ä»£
        search_url = f"https://www.baidu.com/s?wd={keyword}"
        return {
            "keyword": keyword,
            "search_url": search_url,
            "note": "å®Œæ•´è¶‹åŠ¿æ•°æ®éœ€ä½¿ç”¨ç™¾åº¦æŒ‡æ•°APIï¼ˆéœ€æˆæƒï¼‰"
        }


def format_output(data: Dict, output_format: str = "text") -> str:
    """æ ¼å¼åŒ–è¾“å‡º"""
    if not data.get("success"):
        return f"é”™è¯¯: {data.get('error', 'æœªçŸ¥é”™è¯¯')}"

    lines = []
    lines.append(f"ã€{data['category']}ã€‘- {data['fetch_time']}")
    lines.append("-" * 60)

    for item in data["data"]:
        lines.append(f"{item['rank']:2d}. {item['keyword']} (çƒ­åº¦: {item['heat']})")
        lines.append(f"    ğŸ”— {item['url']}")

    if output_format == "json":
        return json.dumps(data, ensure_ascii=False, indent=2)

    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(
        description="ç™¾åº¦çƒ­æœè·å–å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python baidu_hot_search.py                          # è·å–çƒ­æœæ¦œTOP10
  python baidu_hot_search.py --category finance       # è·å–è´¢ç»æ¦œ
  python baidu_hot_search.py --limit 20               # è·å–TOP20
  python baidu_hot_search.py --format json            # JSONæ ¼å¼è¾“å‡º

å¯ç”¨åˆ†ç±»:
  hot(çƒ­æœ) domestic(å›½å†…) abroad(å›½é™…) finance(è´¢ç»)
  sports(ä½“è‚²) entertainment(å¨±ä¹) education(æ•™è‚²)
  tech(ç§‘æŠ€) game(æ¸¸æˆ) car(æ±½è½¦) estate(æˆ¿äº§) travel(æ—…æ¸¸)
        """
    )

    parser.add_argument("-c", "--category", default="hot",
                        help="æ¦œå•åˆ†ç±» (é»˜è®¤: hot)")
    parser.add_argument("-l", "--limit", type=int, default=10,
                        help="è·å–æ•°é‡ (é»˜è®¤: 10)")
    parser.add_argument("-f", "--format", default="text",
                        choices=["text", "json"],
                        help="è¾“å‡ºæ ¼å¼ (é»˜è®¤: text)")
    parser.add_argument("-o", "--output",
                        help="è¾“å‡ºåˆ°æ–‡ä»¶")
    parser.add_argument("-v", "--verbose", action="store_true",
                        help="æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯")

    args = parser.parse_args()

    # åˆå§‹åŒ–è·å–å™¨
    hot_search = BaiduHotSearch()

    # è·å–æ•°æ®
    result = hot_search.get_hot_list(args.category, args.limit)

    # è¾“å‡º
    output = format_output(result, args.format)

    if args.verbose:
        output = f"æ¥æº: {result.get('source', 'unknown')}\n" + output

    print(output)

    # ä¿å­˜åˆ°æ–‡ä»¶
    if args.output:
        with open(args.output, "w", encoding="utf-8") as f:
            f.write(output)
        print(f"\nå·²ä¿å­˜åˆ°: {args.output}")

    # è¿”å›JSONæ ¼å¼ä¾›å…¶ä»–ç¨‹åºä½¿ç”¨
    if args.format == "json":
        return result

    return result


if __name__ == "__main__":
    main()
