#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çœŸå®çƒ­ç‚¹è·å–å™¨ - 0è´¹ç”¨æ–¹æ¡ˆ
æ•°æ®æºï¼šè™å—…ç½‘ + 36æ°ª + å¾®åšçƒ­æœ(å¤‡ç”¨)
"""

import argparse
import json
import re
import sys
from datetime import datetime
from typing import List, Dict, Optional

try:
    import requests
    HAS_DEPS = True
except ImportError:
    HAS_DEPS = False


# ==================== ç”µå•†è€æ¿ç—›ç‚¹æ˜ å°„è¡¨ ====================
# å°†é€šç”¨çƒ­ç‚¹æ˜ å°„åˆ°è€æ¿ç—›ç‚¹

PAIN_POINT_MAPPING = {
    "åˆ©æ¶¦è–„": ["èµšé’±", "ç›ˆåˆ©", "åˆ©æ¶¦", "äºæŸ", "äºæœ¬", "æ”¶å…¥", "è¥æ”¶", "ç”Ÿæ„", "è®¢å•"],
    "æµé‡è´µ": ["æµé‡", "è·å®¢", "æ¨å¹¿", "å¹¿å‘Š", "è¥é”€", "æ›å…‰"],
    "æˆæœ¬é«˜": ["æˆæœ¬", "è´¹ç”¨", "æ¶¨ä»·", "ä»·æ ¼", "æˆ¿ç§Ÿ", "äººå·¥", "å·¥èµ„", "ç§Ÿé‡‘"],
    "è½¬åŒ–éš¾": ["è½¬åŒ–", "é”€å”®", "è´­ä¹°", "ä¸‹å•", "æˆäº¤"],
    "å¹³å°å‹æ¦¨": ["å¹³å°", "è§„åˆ™", "æŠ½æˆ", "ä½£é‡‘", "å°å·", "ç›‘ç®¡"],
    "AIç„¦è™‘": ["AI", "äººå·¥æ™ºèƒ½", "è‡ªåŠ¨åŒ–", "æ™ºèƒ½", "æ›¿ä»£", "è£å‘˜"],
    "èµ„é‡‘å‹åŠ›": ["èµ„é‡‘", "èèµ„", "è´·æ¬¾", "è´¦æœŸ", "å›æ¬¾", "ç°é‡‘æµ", "å€ºåŠ¡"],
    "æ¶ˆè´¹é™çº§": ["æ¶ˆè´¹", "ç»æµ", "é™çº§", "é€šç¼©", "çœé’±"],
}


class RealHotSpotAnalyzer:
    """çœŸå®çƒ­ç‚¹åˆ†æå™¨"""

    def __init__(self, timeout: int = 30):
        self.timeout = timeout
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        })

    def _fetch_page(self, url: str) -> Optional[str]:
        """è·å–é¡µé¢HTML"""
        try:
            response = self.session.get(url, timeout=self.timeout)
            response.encoding = "utf-8"
            return response.text
        except Exception as e:
            print(f"è¯·æ±‚å¤±è´¥: {e}", file=sys.stderr)
            return None

    def _map_to_ecommerce_pain(self, title: str) -> List[str]:
        """å°†çƒ­ç‚¹æ˜ å°„åˆ°ç”µå•†è€æ¿ç—›ç‚¹"""
        tags = []
        for pain_point, keywords in PAIN_POINT_MAPPING.items():
            for kw in keywords:
                if kw in title:
                    tags.append(pain_point)
                    break
        if not tags:
            tags = ["ä¸€èˆ¬çƒ­ç‚¹"]
        return tags

    def _extract_from_huxiu(self) -> List[Dict]:
        """ä»è™å—…ç½‘è·å–çœŸå®çƒ­ç‚¹"""
        try:
            url = "https://www.huxiu.com"
            html = self._fetch_page(url)
            if not html:
                return []

            # è§£æçƒ­ç‚¹æ–‡ç« æ ‡é¢˜
            soup = BeautifulSoup(html, "html.parser")
            articles = []

            # æŸ¥æ‰¾æ–‡ç« æ ‡é¢˜
            for a in soup.find_all("a", href=re.compile(r'/article/\d+')):
                title = a.get_text(strip=True)
                if title and 5 <= len(title) <= 50:
                    # è¿‡æ»¤éçƒ­ç‚¹ç±»
                    exclude = ["è§†é¢‘", "ç›´æ’­", "ä¸“é¢˜", "ä¸“æ ", "æ´»åŠ¨"]
                    if not any(e in title for e in exclude):
                        articles.append({
                            "keyword": title,
                            "source": "è™å—…",
                            "tags": self._map_to_ecommerce_pain(title),
                            "url": "https://www.huxiu.com" + a.get("href", "")
                        })

            # å»é‡
            seen = set()
            unique = []
            for art in articles:
                if art["keyword"] not in seen:
                    seen.add(art["keyword"])
                    unique.append(art)

            return unique[:20]

        except Exception as e:
            print(f"è™å—…è§£æå¤±è´¥: {e}", file=sys.stderr)
            return []

    def _get_36kr_hot(self) -> List[Dict]:
        """ä»36æ°ªè·å–çœŸå®çƒ­ç‚¹"""
        try:
            url = "https://36kr.com/hot-news"
            html = self._fetch_page(url)
            if not html:
                return []

            soup = BeautifulSoup(html, "html.parser")
            articles = []

            for a in soup.find_all("a", href=re.compile(r'/p/\d+')):
                title = a.get_text(strip=True)
                if title and 5 <= len(title) <= 50:
                    articles.append({
                        "keyword": title,
                        "source": "36æ°ª",
                        "tags": self._map_to_ecommerce_pain(title),
                        "url": "https://36kr.com" + a.get("href", "")
                    })

            # å»é‡
            seen = set()
            unique = []
            for art in articles:
                if art["keyword"] not in seen:
                    seen.add(art["keyword"])
                    unique.append(art)

            return unique[:15]

        except Exception as e:
            print(f"36æ°ªè§£æå¤±è´¥: {e}", file=sys.stderr)
            return []

    def get_ecommerce_hotspots(self, limit: int = 15) -> Dict:
        """è·å–ç”µå•†ç›¸å…³çƒ­ç‚¹"""
        # è·å–å¤šä¸ªæ•°æ®æº
        huxiu_data = self._extract_from_huxiu()
        kr36_data = self._get_36kr_hot()

        # åˆå¹¶
        all_data = huxiu_data + kr36_data

        if not all_data:
            return {
                "success": False,
                "error": "æœªèƒ½è·å–åˆ°ä»»ä½•çƒ­ç‚¹æ•°æ®",
                "suggestion": "è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•"
            }

        # æ ‡è®°ç—›ç‚¹å¹¶è¯„åˆ†
        scored = []
        for item in all_data:
            pain_score = 0
            for tag in item.get("tags", []):
                if tag in ["åˆ©æ¶¦è–„", "æµé‡è´µ", "æˆæœ¬é«˜"]:
                    pain_score += 25
                elif tag in ["å¹³å°å‹æ¦¨", "èµ„é‡‘å‹åŠ›", "AIç„¦è™‘"]:
                    pain_score += 20
                elif tag in ["è½¬åŒ–éš¾", "æ¶ˆè´¹é™çº§"]:
                    pain_score += 15

            # ç”µå•†ç›¸å…³å…³é”®è¯åŠ æƒ
            ecommerce_keywords = ["ç”µå•†", "è·¨å¢ƒ", "æ¶ˆè´¹", "é›¶å”®", "å•†ä¸š", "ä¼ä¸š", "è€æ¿", "å¹³å°"]
            for kw in ecommerce_keywords:
                if kw in item["keyword"]:
                    pain_score += 15
                    break

            scored.append({
                **item,
                "pain_score": min(pain_score, 100)
            })

        # æŒ‰ç—›ç‚¹åˆ†æ•°æ’åº
        scored.sort(key=lambda x: x["pain_score"], reverse=True)

        return {
            "success": True,
            "fetch_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "sources": ["è™å—…", "36æ°ª"],
            "total": len(scored),
            "data": scored[:limit]
        }


def format_output(data: Dict) -> str:
    """æ ¼å¼åŒ–è¾“å‡º"""
    if not data.get("success"):
        return f"é”™è¯¯: {data.get('error', 'æœªçŸ¥é”™è¯¯')}\nå»ºè®®: {data.get('suggestion', '')}"

    lines = []
    lines.append("=" * 70)
    lines.append("  ç”µå•†è€æ¿å…³è”çƒ­ç‚¹é€Ÿé€’")
    lines.append("  ç”¨æˆ·ç”»åƒ: [è¢«åˆ©æ¶¦æä½å–‰å’™çš„è€æ¿]")
    lines.append("=" * 70)
    lines.append(f"\nâ° {data['fetch_time']}")
    lines.append(f"ğŸ“¡ æ•°æ®æ¥æº: {', '.join(data['sources'])}")
    lines.append("-" * 70)

    for i, item in enumerate(data.get("data", []), 1):
        tags = item.get("tags", [])[:2]
        lines.append(f"\n{i:2d}. {item['keyword']}")
        lines.append(f"    æ¥æº: {item['source']}")
        lines.append(f"    å…³è”ç—›ç‚¹: {', '.join(tags) if tags else 'ä¸€èˆ¬'}")
        lines.append(f"    ç›¸å…³åº¦: {'â–ˆ' * (item['pain_score'] // 10)}{'â–‘' * (10 - item['pain_score'] // 10)} {item['pain_score']}åˆ†")

    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(
        description="ç”µå•†è€æ¿çƒ­ç‚¹åˆ†æå™¨ - çœŸå®æ•°æ®æº",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    parser.add_argument("-l", "--limit", type=int, default=15,
                        help="è¿”å›æ•°é‡")
    parser.add_argument("-f", "--format", default="text",
                        choices=["text", "json"],
                        help="è¾“å‡ºæ ¼å¼")
    parser.add_argument("-o", "--output",
                        help="è¾“å‡ºåˆ°æ–‡ä»¶")

    args = parser.parse_args()

    if not HAS_DEPS:
        print("è¯·å®‰è£…ä¾èµ–: pip install requests beautifulsoup4 lxml")
        sys.exit(1)

    analyzer = RealHotSpotAnalyzer()
    result = analyzer.get_ecommerce_hotspots(args.limit)

    output = format_output(result)
    if args.format == "json":
        output = json.dumps(result, ensure_ascii=False, indent=2)

    print(output)

    if args.output:
        with open(args.output, "w", encoding="utf-8") as f:
            f.write(output)
        print(f"\nå·²ä¿å­˜åˆ°: {args.output}")


if __name__ == "__main__":
    main()
